package de.kp.works.text.embeddings
/*
 * Copyright (c) 2019 - 2021 Dr. Krusche & Partner PartG. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 * 
 * @author Stefan Krusche, Dr. Krusche & Partner PartG
 * 
 */

import com.johnsnowlabs.nlp.AnnotatorType.{DOCUMENT, TOKEN, WORD_EMBEDDINGS}
import com.johnsnowlabs.nlp._
import com.johnsnowlabs.nlp.annotators.common._
import com.johnsnowlabs.nlp.util.io.ResourceHelper.spark.implicits._
import org.apache.hadoop.fs.Path
import org.apache.spark.ml.SparkParamsReader
import org.apache.spark.ml.util._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

case class Word2VecData(word: String, vector: Array[Float])

class Word2VecModel(override val uid: String, vocab:Map[String, Array[Float]]) extends AnnotatorModel[Word2VecModel]
    with Word2VecParams {

  def this(vocab:Map[String, Array[Float]]) = this(Identifiable.randomUID("WORD2VEC_EMBEDDINGS_MODEL"), vocab)

  def this(uid:String) = this(uid, Map.empty[String, Array[Float]])

  def this() = this(Identifiable.randomUID("WORD2VEC_EMBEDDINGS_MODEL"), Map.empty[String, Array[Float]])

  override val outputAnnotatorType: AnnotatorType = WORD_EMBEDDINGS

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val inputAnnotatorTypes: Array[String] = Array(DOCUMENT, TOKEN)
  
  /*
   * This method is responsible for storing the vocabulary
   * of the Word2Vec Model
   */
  override protected def onWrite(path: String, spark: SparkSession): Unit = {

      import spark.implicits._
      
      val dataPath = s"$path/data"
      
      spark.createDataset[(String, Array[Float])](vocab.toSeq)
        .repartition(1)
        .map { case (word, vector) => Word2VecData(word, vector) }
        .toDF()
        .write
        .parquet(dataPath)
        
  }

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    *
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {

    val sentences = TokenizedWithSentence.unpack(annotations)
    val withEmbeddings = sentences.map{ s =>
      val tokens = s.indexedTokens.map { token =>
        val vectorOption = this.vocab.get(token.token)
        TokenPieceEmbeddings(
          token.token, token.token, -1, isWordStart = true,
          vectorOption, Array.fill[Float]($(vectorSize))(0f), token.begin, token.end)
      }
      WordpieceEmbeddingsSentence(tokens, s.sentenceIndex)
    }

    WordpieceEmbeddingsSentence.pack(withEmbeddings)

  }

  override protected def afterAnnotate(dataset: DataFrame): DataFrame = {
    dataset.withColumn(getOutputCol, wrapEmbeddingsMetadata(dataset.col(getOutputCol), $(vectorSize)))
  }

}

trait EmbeddingsCoverage extends MLReadable[Word2VecModel] {

  case class CoverageResult(covered: Long, total: Long, percentage: Float)

  def withCoverageColumn(dataset: DataFrame, embeddingsCol: String, outputCol: String = "coverage"): DataFrame = {
    val coverageFn = udf((annotatorProperties: Seq[Row]) => {
      val annotations = annotatorProperties.map(Annotation(_))
      val oov = annotations.map(x => if (x.metadata.getOrElse("isOOV", "false") == "false") 1 else 0)
      val covered = oov.sum
      val total = annotations.count(_ => true)
      val percentage = 1f * covered / total
      CoverageResult(covered, total, percentage)
    })
    dataset.withColumn(outputCol, coverageFn(col(embeddingsCol)))
  }

  def overallCoverage(dataset: DataFrame, embeddingsCol: String): CoverageResult = {
    val words = dataset.select(embeddingsCol).flatMap(row => {
      val annotations = row.getAs[Seq[Row]](embeddingsCol)
      annotations.map(annotation => Tuple2(
        annotation.getAs[Map[String, String]]("metadata")("token"),
        if (annotation.getAs[Map[String, String]]("metadata").getOrElse("isOOV", "false") == "false") 1 else 0))
    })
    val oov = words.reduce((a, b) => Tuple2("Total", a._2 + b._2))
    val covered = oov._2
    val total = words.count()
    val percentage = 1f * covered / total
    CoverageResult(covered, total, percentage)
  }
}

object Word2VecModel extends EmbeddingsCoverage {
  
  private class Word2VecModelReader extends MLReader[Word2VecModel] {

    private val className = classOf[Word2VecModel].getName
    
    override def load(path: String): Word2VecModel = {
 
      val spark = sparkSession
      import spark.implicits._

      val metadata = SparkParamsReader.loadMetadata(path, sc, className)

      val dataPath = new Path(path, "data").toString
      val data = spark.read.parquet(dataPath).as[Word2VecData]
      
      val vocab = data.collect.map(item => (item.word, item.vector)).toMap
      val model = new Word2VecModel(metadata.uid, vocab)

      SparkParamsReader.getAndSetParams(model, metadata)
      model
      
    }
  }
  
  override def read: MLReader[Word2VecModel] = new Word2VecModelReader

  override def load(path: String): Word2VecModel = super.load(path)
  
}

